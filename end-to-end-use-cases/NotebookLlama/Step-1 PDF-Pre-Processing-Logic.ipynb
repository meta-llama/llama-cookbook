{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f67a6a6",
   "metadata": {},
   "source": [
    "## Notebook 1: PDF Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68aee84-04e3-4cbc-be78-6de9e06e704f",
   "metadata": {},
   "source": [
    "In the series, we will be going from a PDF to Podcast using all open models. \n",
    "\n",
    "The first step in getting to the podcast is finding a script, right now our logic is:\n",
    "- Use any PDF on any topic\n",
    "- Prompt `Llama-3.2-1B-Instruct` (or Llama-8B model if you're using the API-just uncomment it) to process it into a text file\n",
    "- Re-write this into a podcast transcript in next notebook.\n",
    "\n",
    "In this notebook, we will upload a PDF and save it into a `.txt` file using the `PyPDF2` library, later we will process chunks from the text file using our featherlight model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb3584",
   "metadata": {},
   "source": [
    "Most of us shift-enter pass the comments to realise later we need to install libraries. For the few that read the instructions, please remember to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4fc7aef-3505-482e-a998-790b8b9d48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2\n",
    "# !pip install rich ipywidgets\n",
    "# !pip install llama-api-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23d509",
   "metadata": {},
   "source": [
    "Assuming you have a PDF uploaded on the same machine, please set the path for the file. \n",
    "\n",
    "Also, if you want to flex your GPU-please switch to a bigger model although the featherlight models work perfectly for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d0061b-8b8c-4353-850f-f19466a0ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './resources/2407.21783v3.pdf'\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21029232-ac5f-42ca-b26b-baad5b2f49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from typing import Optional\n",
    "import os\n",
    "from llama_api_client import LlamaAPIClient\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "# Initialize the Llama API client\n",
    "import os\n",
    "os.environ[\"LLAMA_API_KEY\"] = \"api-key\"\n",
    "client = LlamaAPIClient()\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c22eb",
   "metadata": {},
   "source": [
    "Let's make sure we don't stub our toe by checking if the file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153d9ece-37a4-4fff-a8e8-53f923a2b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pdf(file_path: str) -> bool:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return False\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        print(\"Error: File is not a PDF\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a362ac3",
   "metadata": {},
   "source": [
    "Convert PDF to a `.txt` file. This would simply read and dump the contents of the file. We set the maximum characters to 100k. \n",
    "\n",
    "For people converting their favorite novels into a podcast, they will have to add extra logic of going outside the Llama models context length which is 128k tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57c2d64-3d75-4aeb-b4ee-bd1661286b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path: str, max_chars: int = 100000):\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Get total number of pages\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Processing PDF with {num_pages} pages...\")\n",
    "            \n",
    "            extracted_text = []\n",
    "            total_chars = 0\n",
    "            \n",
    "            # Iterate through all pages\n",
    "            for page_num in range(num_pages):\n",
    "                # Extract text from page\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                \n",
    "                # Check if adding this page's text would exceed the limit\n",
    "                if total_chars + len(text) > max_chars:\n",
    "                    # Only add text up to the limit\n",
    "                    remaining_chars = max_chars - total_chars\n",
    "                    extracted_text.append(text[:remaining_chars])\n",
    "                    print(f\"Reached {max_chars} character limit at page {page_num + 1}\")\n",
    "                    break\n",
    "                \n",
    "                extracted_text.append(text)\n",
    "                total_chars += len(text)\n",
    "                print(f\"Processed page {page_num + 1}/{num_pages}\")\n",
    "            \n",
    "            final_text = '\\n'.join(extracted_text)\n",
    "            print(f\"\\nExtraction complete! Total characters: {len(final_text)}\")\n",
    "            return final_text\n",
    "            \n",
    "    except PyPDF2.PdfReadError:\n",
    "        print(\"Error: Invalid or corrupted PDF file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023397b",
   "metadata": {},
   "source": [
    "Helper function to grab meta info about our PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0984bb1e-d52c-4cec-a131-67a48061fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF metadata\n",
    "def get_pdf_metadata(file_path: str):\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            metadata = {\n",
    "                'num_pages': len(pdf_reader.pages),\n",
    "                'metadata': pdf_reader.metadata\n",
    "            }\n",
    "            return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019affc",
   "metadata": {},
   "source": [
    "Finally, we can run our logic to extract the details from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63848943-79cc-4e21-8396-6eab5df493e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata...\n",
      "\n",
      "PDF Metadata:\n",
      "Number of pages: 92\n",
      "Document info:\n",
      "/Author: \n",
      "/CreationDate: D:20241126014049Z\n",
      "/Creator: LaTeX with hyperref\n",
      "/Keywords: \n",
      "/ModDate: D:20241126014049Z\n",
      "/PTEX.Fullbanner: This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\n",
      "/Producer: pdfTeX-1.40.25\n",
      "/Subject: \n",
      "/Title: \n",
      "/Trapped: /False\n",
      "\n",
      "Extracting text...\n",
      "Processing PDF with 92 pages...\n",
      "Processed page 1/92\n",
      "Processed page 2/92\n",
      "Processed page 3/92\n",
      "Processed page 4/92\n",
      "Processed page 5/92\n",
      "Processed page 6/92\n",
      "Processed page 7/92\n",
      "Processed page 8/92\n",
      "Processed page 9/92\n",
      "Processed page 10/92\n",
      "Processed page 11/92\n",
      "Processed page 12/92\n",
      "Processed page 13/92\n",
      "Processed page 14/92\n",
      "Processed page 15/92\n",
      "Processed page 16/92\n",
      "Processed page 17/92\n",
      "Processed page 18/92\n",
      "Processed page 19/92\n",
      "Processed page 20/92\n",
      "Processed page 21/92\n",
      "Processed page 22/92\n",
      "Processed page 23/92\n",
      "Processed page 24/92\n",
      "Processed page 25/92\n",
      "Processed page 26/92\n",
      "Reached 100000 character limit at page 27\n",
      "\n",
      "Extraction complete! Total characters: 100026\n",
      "\n",
      "Preview of extracted text (first 500 characters):\n",
      "--------------------------------------------------\n",
      "The Llama 3 Herd of Models\n",
      "Llama Team, AI @ Meta1\n",
      "1A detailed contributor list can be found in the appendix of this paper.\n",
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a\n",
      "new set of foundation models, called Llama 3. It is a herd of language models that natively support\n",
      "multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with\n",
      "405B parameters and a context window of up to 128K tokens. This paper presents \n",
      "--------------------------------------------------\n",
      "\n",
      "Total characters extracted: 100026\n",
      "\n",
      "Extracted text has been saved to ./resources/extracted_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata first\n",
    "print(\"Extracting metadata...\")\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "if metadata:\n",
    "    print(\"\\nPDF Metadata:\")\n",
    "    print(f\"Number of pages: {metadata['num_pages']}\")\n",
    "    print(\"Document info:\")\n",
    "    for key, value in metadata['metadata'].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Extract text\n",
    "print(\"\\nExtracting text...\")\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Display first 500 characters of extracted text as preview\n",
    "if extracted_text:\n",
    "    print(\"\\nPreview of extracted text (first 500 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(extracted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nTotal characters extracted: {len(extracted_text)}\")\n",
    "\n",
    "# Optional: Save the extracted text to a file\n",
    "if extracted_text:\n",
    "    output_file = './resources/extracted_text.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "    print(f\"\\nExtracted text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d1f59",
   "metadata": {},
   "source": [
    "### Llama Pre-Processing\n",
    "\n",
    "Now let's proceed to justify our distaste for writing regex and use that as a justification for a LLM instead:\n",
    "\n",
    "At this point, have a text file extracted from a PDF of a paper. Generally PDF extracts can be messy due to characters, formatting, Latex, Tables, etc. \n",
    "\n",
    "One way to handle this would be using regex, instead we can also prompt the feather light Llama models to clean up our text for us. \n",
    "\n",
    "Please try changing the `SYS_PROMPT` below to see what improvements you can make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0828a5-964d-475e-b5f5-40a04e287725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n",
    "\n",
    "The raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n",
    "\n",
    "Remember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n",
    "\n",
    "Please be smart with what you remove and be creative ok?\n",
    "\n",
    "Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED\n",
    "\n",
    "Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n",
    "\n",
    "PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES\n",
    "\n",
    "ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\n",
    "Here is the text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd393fae",
   "metadata": {},
   "source": [
    "Instead of having the model process the entire file at once, as you noticed in the prompt-we will pass chunks of the file. \n",
    "\n",
    "One issue with passing chunks counted by characters is, we lose meaning of words so instead we chunk by words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e8a547-9d7c-4e2f-be9e-a3aea09cce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_bounded_chunks(text, target_chunk_size):\n",
    "    \"\"\"\n",
    "    Split text into chunks at word boundaries close to the target chunk size.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_length = len(word) + 1  # +1 for the space\n",
    "        if current_length + word_length > target_chunk_size and current_chunk:\n",
    "            # Join the current chunk and add it to chunks\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74223f",
   "metadata": {},
   "source": [
    "Let's load in the model and start processing the text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d04a4f07-b0b3-45ca-8f41-a433e1abe050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator = Accelerator()\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     DEFAULT_MODEL,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     use_safetensors=True,\n",
    "#     device_map=device,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "# model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbda5241-e890-4402-87dd-514d6761bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(text_chunk, chunk_num):\n",
    "    \"\"\"Process a chunk of text using Llama API and return output\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Llama-3.3-8B-Instruct\",  # Use the appropriate model ID\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": SYS_PROMPT\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": text_chunk\n",
    "            }\n",
    "        ],\n",
    "        max_completion_tokens=512,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    processed_text = response.completion_message.content.text\n",
    "    \n",
    "    # Print chunk information for monitoring\n",
    "    print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n",
    "    print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n",
    "    print(f\"{'='*90}\\n\")\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aadf30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code block before creating chunks:\n",
    "# Load the extracted text from file\n",
    "with open('./resources/extracted_text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "INPUT_FILE = \"./resources/extracted_text.txt\"  # Replace with your file path\n",
    "CHUNK_SIZE = 1000  # Adjust chunk size if needed\n",
    "\n",
    "chunks = create_word_bounded_chunks(text, CHUNK_SIZE)\n",
    "num_chunks = len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb36814f-9310-4734-bf54-e16a5032339e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "447188d3-ebf0-42d5-940e-4d7e0d9dbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Calculate number of chunks\n",
    "num_chunks = (len(text) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "\n",
    "# Cell 6: Process the file with ordered output\n",
    "# Create output file name\n",
    "output_file = f\"clean_{os.path.basename(INPUT_FILE)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7917dfdd-b3af-44fc-a8c0-2760ace9363e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a9ffc357cf434f9583be2151572655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "The Llama 3 Herd of Models Llama Team, AI @ Meta1 1A detailed contributor list can be found in the appendix of this paper. Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Modern artificial intelligence systems are powered by foundation models. This paper presents a new set of foundation models called Llama 3. It is a herd of language models that support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. Llama 3 delivers comparable quality to leading language models on a plethora of tasks. We publicly release Llama 3 including pre-trained and post-trained vers...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development. Date:July 23, 2024 Website: https://llama.meta.com/ 1 Introduction Foundation models are general models of language, vision, speech, and/or other modalities that are designed to support a large variety of AI tasks. They form the basis of many modern AI systems. The development of modern foundatio...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development. \n",
      "Foundation models are general models of language, vision, speech, and/or other modalities that are designed to support a large variety of AI tasks. They form the basis of many modern AI systems. \n",
      "The development of modern foundation models consists of two main stages: a pre-training stage and a ...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "multilinguality, coding, reasoning, and tool usage. Our largest model is dense Transformer with 405B parameters, processing information in a context window of up to 128K tokens. Each member of the herd is listed in Table 1. All the results presented in this paper are for the Llama 3.1 models, which we will refer to as Llama 3 throughout for brevity. We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimiz...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Our largest model is a dense Transformer with 405B parameters, processing information in a context window of up to 128K tokens. We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process. \n",
      "We improved the quantity and quality of the data we use for pre-training and post-training compared to prior versions of Llama. This includes more careful pre-processing ...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "compared to 1.8T tokens for Llama 2. •Scale.We train a model at far larger scale than previous Llama models: our flagship language model was pre-trained using 3.8×1025FLOPs, almost 50×more than the largest version of Llama 2. Specifically, we pre-trained a flagship model with 405B trainable parameters on 15.6T text tokens. As expected per 1arXiv:2407.21783v3 [cs.AI] 23 Nov 2024 Finetuned Multilingual Long context Tool use Release Llama 3 8B ✗ ✗1✗ ✗ April 2024 Llama 3 8B Instruct ✓ ✗ ✗ ✗ April 20...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "We train a model at a larger scale than previous Llama models. Our flagship language model was pre-trained using a large amount of computational power and 405B trainable parameters on 15.6T text tokens. This is significantly larger than the largest version of Llama 2. All results in this paper are for the Llama 3.1 models....\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "same procedure. While our scaling laws suggest our flagship model is an approximately compute-optimal size for our training budget, we also train our smaller models for much longer than is compute-optimal. The resulting models perform better than compute-optimal models at the same inference budget. We use the flagship model to further improve the quality of those smaller models during post-training. •Managing complexity. We make design choices that seek to maximize our ability to scale the model...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "We scale our model to an approximately compute-optimal size for our training budget. We also train smaller models for longer than is compute-optimal. These models perform better than compute-optimal models at the same inference budget. We use the flagship model to improve the quality of smaller models after training. \n",
      "\n",
      "We make design choices to maximize our ability to scale the model development process. We use a standard dense Transformer model architecture with minor adaptations. We adopt a si...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "et al., 2022; Schulman et al., 2017) that tend to be less stable and harder to scale. The result of our work is Llama 3: a herd of three multilingual1language models with 8B, 70B, and 405B parameters. We evaluate the performance of Llama 3 on a plethora of benchmark datasets that span a wide range of language understanding tasks. In addition, we perform extensive human evaluations that compare Llama 3 with competing models. An overview of the performance of the flagship Llama 3 model on key benc...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Our work results in Llama 3, a herd of three multilingual language models with 8B, 70B, and 405B parameters. We evaluate Llama 3 on various benchmark datasets and perform human evaluations comparing it to competing models. The flagship model performs on par with leading language models across different tasks and matches the state-of-the-art. Smaller models outperform alternatives with similar parameters. Llama 3 also offers a better balance between being helpful and harmless than its predecessor...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "et al., 2023b). We present a detailed analysis of the safety of Llama 3 in Section 5.4. We are publicly releasing all three Llama 3 models under an updated version of the Llama 3 Community License; seehttps://llama.meta.com . This includes pre-trained and post-trained versions of our 405B parameter language model and a new version of our Llama Guard model (Inan et al., 2023) for input and output safety. We hope that the open release of a flagship model will spur a wave of innovation in the resea...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "We present a detailed analysis of the safety of Llama 3 in Section 5.4. We are releasing all three Llama 3 models under an updated version of the Llama 3 Community License. This includes pre-trained and post-trained versions of our 405B parameter language model and a new version of our Llama Guard model for input and output safety. The open release of the model is hoped to spur innovation in the research community and accelerate a responsible path towards artificial general intelligence. As part...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "models. 1The Llama 3 8B and 70B were pre-trained on multilingual data but were intended for use in English at the time. 2 Category Benchmark Llama 3 8B Gemma 2 9B Mistral 7B Llama 3 70B Mixtral 8x22B GPT 3.5 Turbo Llama 3 405B Nemotron 4 340B GPT-4 (0125) GPT-4o Claude 3.5 Sonnet GeneralMMLU (5-shot) 69.4 72.361.1 83.676.970.787.382.6 85.189.1 89.9 MMLU (0-shot, CoT) 73.0 72.3△60.5 86.079.969.888.6 78.7◁85.4 88.7 88.3 MMLU-Pro (5-shot, CoT) 48.3 –36.9 66.456.349.273.362.7 64.874.0 77.0 IFEval 80...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Llama 3 8B and Llama 3 70B were pre-trained on multilingual data for use in English. \n",
      "Category Benchmark results: \n",
      "Llama 3 8B, Gemma 2 9B, Mistral 7B, Llama 3 70B, Mixtral 8x22B, GPT 3.5 Turbo, Llama 3 405B, Nemotron 4 340B, GPT-4, Claude 3.5, Sonnet, GeneralMMLU, MMLU (0-shot, CoT), MMLU-Pro (5-shot, CoT), IFEval, CodeHumanEval (0-shot), MBPP EvalPlus (0-shot), MathGSM8K, MATH (0-shot, CoT), ReasoningARC Challenge, GPQA (0-shot, CoT), Tool useBFCL. \n",
      "Results: \n",
      "69.4, 72.36, 11.1, 83.67, 6.97, 70....\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "76.1 –60.484.8– 85.988.586.5 88.380.5 90.2 Nexus 38.5 30.024.7 56.748.537.2 58.7 –50.356.1 45.7 Long contextZeroSCROLLS/QuALITY 81.0 ––90.5–– 95.2 – 95.2 90.5 90.5 InfiniteBench/En.MC 65.1 ––78.2–– 83.4 –72.182.5 – NIH/Multi-needle 98.8 ––97.5––98.1 – 100.0 100.0 90.8 Multilingual MGSM (0-shot, CoT) 68.9 53.229.9 86.971.151.4 91.6 –85.990.5 91.6 Table 2 Performance of finetuned Llama 3 models on key benchmark evaluations. The table compares the performance of the 8B, 70B, and 405B versions of Ll...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "The development of our Llama 3 language models comprises two main stages: \n",
      "Language model pre-training. We start by converting a large, multilingual text corpus to \n",
      "fine-tune the models on key benchmark evaluations. The table compares the performance of the 8B, 70B, and 405B versions of Llama 3 with that of competing models....\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "discrete tokens and pre-training a large language model (LLM) on the resulting data to perform next-token prediction. In the language model pre-training stage, the model learns the structure of language and obtains large amounts of knowledge about the world from the text it is “reading”. To do this effectively, pre-training is performed at massive scale: we pre-train a model with 405B parameters on 15.6T tokens using a context window of 8K tokens. This standard pre-training stage is followed by ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "pre-training a large language model on text data enables it to learn language structure and gain knowledge about the world. This pre-training is done on a massive scale with a model having 405B parameters and 15.6T tokens. After the initial pre-training, the model undergoes continued pre-training to increase its context window. \n",
      "The pre-trained model then requires post-training to align with human expectations and follow instructions. This involves supervised finetuning and Direct Preference Opt...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "al., 2024). At this post-training2stage, we also integrate new capabilities, such as tool-use, and observe strong improvements in other areas, such as coding and reasoning. See Section 4 for details. Finally, safety mitigations are also incorporated into the model at the post-training stage, the details of which are described in Section 5.4. The resulting models have a rich set of capabilities. They can answer questions in at least eight languages, write high-quality code, solve complex reasonin...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "We integrate new capabilities such as tool-use at the post-training stage, resulting in strong improvements in areas like coding and reasoning. The models can answer questions in multiple languages, write high-quality code, solve complex problems, and use tools. We also add image, video, and speech capabilities to Llama 3 using a compositional approach. This approach includes training separate encoders for images and speech, teaching the model the relationship between visual content and text....\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "description of that content in natural language. Our speech encoder is trained using a 2In this paper, we use the term “post-training” to refer to any model training that happens outside of pre-training. 3 Figure 1 Illustration of the overall architecture and training of Llama 3. Llama 3 is a Transformer language model trained to predict the next token of a textual sequence. See text for details. self-supervised approach that masks out parts of the speech inputs and tries to reconstruct the mask...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "We use the term post-training to refer to any model training that happens outside of pre-training. Llama 3 is a Transformer language model trained to predict the next token of a textual sequence. The model learns the structure of speech signals using a self-supervised approach that masks out parts of the speech inputs and tries to reconstruct the masked out parts. We also train an adapter that integrates a pre-trained image encoder into the pre-trained language model....\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "pairs. This aligns the image representations with the language representations. During adapter training, we also update the parameters of the image encoder but we intentionally do not update the language-model parameters. We also train a video adapter on top of the image adapter on paired video-text data. This enables the model to aggregate information across frames. See Section 7 for details. •Speech adapter training. Finally, we integrate the speech encoder into the model via an adapter that c...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "This aligns the image representations with the language representations. During adapter training, we update the parameters of the image encoder and train a video adapter on paired video-text data to aggregate information across frames. We also integrate the speech encoder into the model via an adapter that converts speech encodings into token representations for the finetuned language model. The parameters of the adapter and encoder are jointly updated for high-quality speech understanding. Our ...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "interaction via a speech interface. These models are still under development and not yet ready for release. 3 Pre-Training Language model pre-training involves: (1)the curation and filtering of a large-scale training corpus, (2)the development of a model architecture and corresponding scaling laws for determining model size, (3)the development of techniques for efficient pre-training at large scale, and (4)the development of a pre-training recipe. We present each of these components separately b...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "We create a dataset for language model pre-training from various data sources up to 2023. We apply de-duplication methods and data cleaning to obtain high-quality tokens. We remove data containing personally identifiable information and adult content. \n",
      "\n",
      "We obtain much of our data from the web and describe...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "our cleaning process below. PII and safety filtering. Among other mitigations, we implement filters designed to remove data from websites are likely to contain unsafe content or high volumes of PII, domains that have been ranked as harmful according to a variety of Meta safety standards, and domains that are known to contain adult content. 4 Text extraction and cleaning. We process the raw HTML content for non-truncated web documents to extract high-quality diverse text. To do so, we build a cus...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Our cleaning process involves PII and safety filtering. We remove data from websites that may contain unsafe content or high volumes of personally identifiable information. We also filter out domains ranked as harmful by Meta's safety standards and those known to contain adult content. \n",
      "\n",
      "Text extraction and cleaning are key steps. We process raw HTML content to extract high-quality, diverse text. Our custom parser is designed to optimize for precision in removing boilerplate and ensuring content...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "pre-rendered images where the math is also provided in the altattribute. We experimentally evaluate different cleaning configurations. We find markdown is harmful to the performance of a model that is primarily trained on web data compared to plain text, so we remove all markdown markers. De-duplication. We apply several rounds of de-duplication at the URL, document, and line level: •URL-level de-duplication. We perform URL-level de-duplication across the entire dataset. We keep the most recent ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "We experimentally evaluate different cleaning configurations. We find that markdown is harmful to the performance of a model trained on web data compared to plain text, so we remove all markdown markers. \n",
      "We apply several rounds of de-duplication at the URL, document, and line level. \n",
      "We keep the most recent version for pages corresponding to each URL. \n",
      "We perform global de-duplication across the entire dataset to remove near duplicate documents. \n",
      "We remove lines that appeared more than 6 times ...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "boilerplate from various websites such as navigation menus, cookie warnings, but also frequent high-quality text, our empirical evaluations showed strong improvements. Heuristic filtering. We develop heuristics to remove additional low-quality documents, outliers, and documents with excessive repetitions. Some examples of heuristics include: •We use duplicated n-gram coverage ratio (Rae et al., 2021) to remove lines that consist of repeated content such as logging or error messages. Those lines ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "We develop heuristics to remove low-quality documents and outliers. Some examples include using duplicated n-gram coverage ratio to remove repeated content and \"dirty word\" counting to filter out adult websites. We also use a token-distribution Kullback-Leibler divergence to filter out documents with excessive outlier tokens. Further, we experiment with model-based quality classifiers to sub-select documents....\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "high-quality tokens. These include using fast classifiers such as fasttext (Joulin et al., 2017) trained to recognize if a given text would be referenced by Wikipedia (Touvron et al., 2023a), as well as more compute-intensive Roberta-based classifiers (Liu et al., 2019a) trained on Llama 2 predictions. To train a quality classifier based on Llama 2, we create a training set of cleaned web documents, describe the quality requirements, and instruct Llama 2’s chat model to determine if the document...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "high-quality tokens include using fast classifiers such as fasttext trained to recognize if a given text would be referenced by Wikipedia, as well as Roberta-based classifiers trained on Llama 2 predictions. To train a quality classifier based on Llama 2, we create a training set of cleaned web documents and instruct Llama 2 to determine if the documents meet the quality requirements. We use DistilRoberta to generate quality scores for each document. We experimentally evaluate the efficacy of va...\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_text = \"\"\n",
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    for chunk_num, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        # Process chunk and append to complete text\n",
    "        processed_chunk = process_chunk(chunk, chunk_num)\n",
    "        processed_text += processed_chunk + \"\\n\"\n",
    "        # Write chunk immediately to file\n",
    "        out_file.write(processed_chunk + \"\\n\")\n",
    "        out_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cffe8d",
   "metadata": {},
   "source": [
    "Let's print out the final processed versions to make sure things look good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89ef51a7-f13f-49a4-8f73-9ac8ce75319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Input file: ./resources/extracted_text.txt\n",
      "Output file: clean_extracted_text.txt\n",
      "Total chunks processed: 101\n",
      "\n",
      "Preview of final processed text:\n",
      "\n",
      "BEGINNING:\n",
      "The Llama 3 Herd of Models is a new set of foundation models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model has 405B parameters and can process up to 128K tokens. This paper evaluates Llama 3 and finds it delivers comparable quality to leading language models on various tasks. Llama 3 is publicly released, including pre-trained and post-trained versions, as well as the Llama Guard 3 model for safety.\n",
      "This approach performs competitively with state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development. \n",
      "Foundation models are general models of language, vision, speech, and other modalities that support a large variety of AI tasks. They form the basis of many modern AI systems. \n",
      "The development of modern foundation models consists of two main stages: a pre-training stage and a post-training stage. \n",
      "We present a new set of foundation models for langua\n",
      "\n",
      "...\n",
      "\n",
      "END:\n",
      "eing overconfident in domains where they have little knowledge.\n",
      "the spread of misinformation. We took a hallucination-first approach here. \n",
      "Our approach involves generating data that aligns model generations with subsets of factual data present in the pre-training data. \n",
      "We develop a knowledge probing technique that takes advantage of the model's in-context abilities. \n",
      "This data generation process involves extracting a data snippet from the pre-training data, generating a factual question about these snippets, sampling responses from the model, and scoring the correctness of the generations.\n",
      "Here is the processed text:\n",
      "\n",
      "We use Llama 3 as a judge to score the informativeness of the generations. The score is 5. A refusal is generated for responses that are consistently informative and incorrect across the generations, as judged by Llama 3. This refusal is used to encourage the model to only answer questions it has knowledge about and to refuse answering those questions it does not know.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"Total chunks processed: {num_chunks}\")\n",
    "\n",
    "# Preview the beginning and end of the complete processed text\n",
    "print(\"\\nPreview of final processed text:\")\n",
    "print(\"\\nBEGINNING:\")\n",
    "print(processed_text[:1000])\n",
    "print(\"\\n...\\n\\nEND:\")\n",
    "print(processed_text[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d996ac5",
   "metadata": {},
   "source": [
    "### Next Notebook: Transcript Writer\n",
    "\n",
    "Now that we have the pre-processed text ready, we can move to converting into a transcript in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b16ae0e-04cf-4eb9-a369-dee1728b89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
